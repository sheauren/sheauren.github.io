Given actor parameter θ
τ^1:
    (s_1^1,a_1^1) R(τ^1)
    (s_2^1,a_2^1) R(τ^1)
    ...
τ^2:
    (s_1^2,a_1^1) R(τ^2)
    (s_2^2,a_2^2) R(τ^2)
    ...
拿上面的Data collection
    update 參數 θ
    θ <- θ + η∇R_θ^T
    ∇R_θ^- = 1/N Σ_n=1^N Σ_t=1^Tn R(τ^n)∇logp(a_t^n|s_t^n,θ)
        R(τ^n) 
            先拿掉當作等於1
        ∇logp(a_t^n|s_t^n,θ)
            input s_1^1
                NN
                    output
                        a_1^1 = left
                            target
                                1 0 0
            input s_1^2
                NN
                    output
                        a_1^2 = fire
                            target
                                0 0 1
            很像分類問題                
                多了reward獎勵R(τ^n)給training example weight
                假如R(τ^i)=2就是複製i兩次去train
                                    

            
上面重複循環

Considered as Classification Problem
    s -> NN -> classifier[left/right/fire]
        [lef/right/fire] = y_i
        target 1,0,0 = y_i^^
        minimum: (crossentropy)
            - Σ_i=1^3 y_i^^ log y_i
                多數y^_i都是0
                    Maximize: logyi = logP("left"|s)
    θ <- θ + η∇logP("left"|s)
        當式子有η∇logP("left"|s)
            我們希望objective function越大越好
                是在解分類問題
                    network output
                        跟target正確類別越接近越好
                