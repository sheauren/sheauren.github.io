python
    conda
        tensorflow
        tensorflow-gpu
    pip
        tensorflow
        numpy
        cv2
        Pillow
        matplotlib            
loss_function
    sparse_categorical_crossentropy
    mean_squared_error:mse
optimizer
    SGDOptimizer
    AdamOptimizer
    RMSPropOptimizer
metrics
    accuracy
    mean_absolute_error:mae
callbacks
    EarlyStopping
    ModelCheckpoint
    class PrintDot(keras.callbacks.Callback):
        def on_epoch_end(self,epoch,logs):
            if epoch % 100 == 0: print('')
            print('.', end='')    
layer
    Dense
    Conv2D
    LSTM
    BatchNormalized
    Dropout
    Custom Layer
        class MyDenseLayer(tf.keras.layers.Layer):
            def __init__(self, num_outputs):
                super(MyDenseLayer, self).__init__()
                self.num_outputs = num_outputs                    
            def build(self, input_shape):
                self.kernel = self.add_variable("kernel", 
                    shape=[input_shape[-1].value, 
                    self.num_outputs])                    
            def call(self, input):
                return tf.matmul(input, self.kernel)
        layer = MyDenseLayer(10)
    Composing Layer
        class ResnetIdentityBlock(tf.keras.Model):
            def __init__(self, kernel_size, filters):
                super(ResnetIdentityBlock, self).__init__(name='')
                filters1, filters2, filters3 = filters
                self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))
                self.bn2a = tf.keras.layers.BatchNormalization()
                self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')
                self.bn2b = tf.keras.layers.BatchNormalization()
                self.conv2c = tf.keras.layers.Conv2D(filters3, (1, 1))
                self.bn2c = tf.keras.layers.BatchNormalization()
            def call(self, input_tensor, training=False):
                x = self.conv2a(input_tensor)
                x = self.bn2a(x, training=training)
                x = tf.nn.relu(x)
                x = self.conv2b(x)
                x = self.bn2b(x, training=training)
                x = tf.nn.relu(x)
                x = self.conv2c(x)
                x = self.bn2c(x, training=training)
                x += input_tensor
                return tf.nn.relu(x)
        block = ResnetIdentityBlock(1, [1, 2, 3])
overfit/underfit prevent
    overfit
        DropOut
        BatchNormalized
        Weight Normalized
        Data Argumentation
    underfit
        more epochs
dataset
    studing dataset
        train_x,train_y=tf.keras.dataset.fashion_mist.load_data()
        train_x,train_y=tf.keras.dataset.imdb.load_data()  
    custom dataset
        tf.data.Dataset.from_tensor_slices()
    transformations
        batch
        map
        shuffle
save/load weight
    save
        all training info
        h5 file
    load_model
        h5 file
    save_weights
        only weight data
        ckpt-checkpoint
        weight        
            file
            weight file
            tflite for deploy
                convert form weight/h5
    load_weights
        ckpt
        h5
        weight
    load pretrain model
        tf.keras.Application.MobileNetV2
        tf.keras.Application.ResNet152V2
enable_eager_execution
    tf.enable_eager_execution()
    gradient function
        tf.contrib.eager.gradients_function(f)
    gradient tape
        with tf.GradientTape() as tape:
            pred = model(input_data), training=True)
            loss = loss_func(pred,target)
        gradients = tape.gradient(loss,model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
gpu
    nvida graphic card
        cuda version
        cudnn version
        tensorflow-gpu in conda
        tensorflow-gpu in tf1.x
train_example:iris
    dataset
        train_dataset_url = "http://download.tensorflow.org/data/iris_training.csv"
        train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),origin=train_dataset_url)
        column_names : ['sepal_length', 'sepal_width', 'petal_length', 'petal_width','species']
        train_dataset = tf.contrib.data.make_csv_dataset(
            train_dataset_fp,
            batch_size, 
            column_names=column_names ,
            label_name=label_name,
            num_epochs=1)
        train_x
            (120,4)
            Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
        train_y
            (120,1)
            Label: species
    model
        model = tf.keras.Sequential([
        tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
        tf.keras.layers.Dense(10, activation=tf.nn.relu),
        tf.keras.layers.Dense(3)])
    Using the model
        predictions = model(features)
        predictions=tf.nn.softmax(predictions[:,:],axis=1)
        predictions=tf.argmax(predictions, axis=1)
    loss
        def loss(model, x, y):
            y_ = model(x)                
            return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)
    gradient decent
        def grad(model, inputs, targets):
            with tf.GradientTape() as tape:
                loss_value = loss(model, inputs, targets)
            return loss_value, tape.gradient(loss_value, model.trainable_variables)
    optimizer 
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
        global_step = tf.train.get_or_create_global_step()
        optimizer.apply_gradients(zip(grads, model.variables), global_step)
    epochs
        train_loss_results = []
        train_accuracy_results = []
        num_epochs = 201
        for epoch in range(num_epochs):
            epoch_loss_avg = tfe.metrics.Mean()
            epoch_accuracy = tfe.metrics.Accuracy()
            for x, y in train_dataset:
            loss_value, grads = grad(model, x, y)
            optimizer.apply_gradients(zip(grads, model.variables),global_step)
            epoch_loss_avg(loss_value)
            epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)
        train_loss_results.append(epoch_loss_avg.result())
        train_accuracy_results.append(epoch_accuracy.result())
        if epoch % 50 == 0:
            print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,
                epoch_loss_avg.result(),
                epoch_accuracy.result()))
    test
        dataset
            test_url = "http://download.tensorflow.org/data/iris_test.csv"
            test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),origin=test_url)
            test_dataset = tf.contrib.data.make_csv_dataset(
                train_dataset_fp,
                batch_size, 
                column_names=column_names,
                label_name='species',
                num_epochs=1,
                shuffle=False)
            test_dataset = test_dataset.map(pack_features_vector)
        testing
            test_accuracy = tfe.metrics.Accuracy()
            for (x, y) in test_dataset:
                logits = model(x)
                prediction = tf.argmax(logits, axis=1, output_type=tf.int32)
                test_accuracy(prediction, y)
            print("Test set accuracy: {:.3%}".format(test_accuracy.result()))
    predict
        predict_dataset = tf.convert_to_tensor([
            [5.1, 3.3, 1.7, 0.5,],
            [5.9, 3.0, 4.2, 1.5,],
            [6.9, 3.1, 5.4, 2.1]
        ])
        predictions = model(predict_dataset)
        for i, logits in enumerate(predictions):
            class_idx = tf.argmax(logits).numpy()
            p = tf.nn.softmax(logits)[class_idx]
            name = class_names[class_idx]
            print("Example {} prediction: {} ({:4.1f}%)".format(i, name, 100*p))
